<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://localhost:4000/blog/</link>
    <description></description>
    <pubDate>Thu, 27 Apr 2017 15:36:04 +0100</pubDate>
    
      <item>
        <title>The Reparamaterisation Trick</title>
        <link>/reperam</link>
        <guid isPermaLink="true">/reperam</guid>
        <description>&lt;p&gt;Machine learning is full of lots of seemingly simple mathematical “tricks” that
have a disproportionate usefulness relative to their complexity. The Reparamaterisation
trick is one of these. In the next few posts I want to go into more detail about
variational auto-encoders and the trick will come up there again but for the time
being I just want to present a neat and useful “trick” that I think can be confusing
when you first come across it.&lt;/p&gt;

&lt;p&gt;Often times in machine learning we want to estimate the gradient of an expectation
using sampling. This happens in reinforcement learning when doing policy gradients and in stochastic
variational inference. The problem we are trying to solve is to find:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta E_{p(x)}[f(x)].&lt;/script&gt;

&lt;p&gt;As long as &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; is independent of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; there’s no problem here, we can
use Leibniz rule and simply bring the derivative into the expectation like follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{p(x)}[\nabla_\theta f_\theta(x)]&lt;/script&gt;

&lt;p&gt;and then construct a monte-carlo estimate for the gradient:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{p(x)}[\nabla_\theta f_\theta(x)] \approx \sum_i \nabla_\theta f_\theta(x_i)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is sampled from &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;. If however, as is often the case, the distribution
depends on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. i.e &lt;script type=&quot;math/tex&quot;&gt;P(x) = P(x|\theta)&lt;/script&gt;, then the above trick wont work because after
you bring the derivative inside the integral (or sum), the new integral is no longer an expectation
and we cant construct a straightforward monte-carlo estimate.&lt;/p&gt;

&lt;p&gt;There are two ways to get around this. The first is used in the Reinforce algorithm and is known
as the log derivative trick. It relies on using the following identity, which is easy to verify:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta g(\theta) \equiv g(\theta)\nabla_\theta log (g(\theta))&lt;/script&gt;

&lt;p&gt;If we substitute this identity for &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta p_\theta(x)&lt;/script&gt; in the following equation then we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta E_{p_\theta(x)}[f(x)] = \int f(x)\nabla_\theta p_\theta(x) dx = \int f(x)p_\theta(x)\nabla_\theta log p_\theta(x) dx = E_{p_{\theta}(x)}[f(x)\nabla_\theta log p_{\theta}(x)]&lt;/script&gt;

&lt;p&gt;and since this is still an expectation we can again construct a simple monte-carlo estimate of the gradient.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{p_{\theta}(x)}[f(x)\nabla_\theta log p_{\theta}(x)] \approx \sum_i f(x_i)\nabla_\theta log p_{\theta}(x_i)&lt;/script&gt;

&lt;p&gt;job done right? well sometimes yes but sometimes this naive substitution yields a gradient estimate that is too high
variance to be useful. In those cases we can turn to the reparamaterisation trick.&lt;/p&gt;

&lt;p&gt;The idea behind the reparamaterisation trick is write &lt;script type=&quot;math/tex&quot;&gt;x \sim p_\theta(x)&lt;/script&gt; as a function of a varible &lt;script type=&quot;math/tex&quot;&gt;z \sim q(z)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;q(z)&lt;/script&gt; does not depend
on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. If we can do this, then we are back in the first situation we considered where the gradient operator can simply be brought into
the expectation without any complexity. Put another way we want to find a differentiable function &lt;script type=&quot;math/tex&quot;&gt;\phi_\theta(z)&lt;/script&gt; such that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_\theta(x) = q(\phi_\theta(z))&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is a distribution that has no &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; dependence. An example of such a function would
be the following reparamaterisation of a normal distribution &lt;script type=&quot;math/tex&quot;&gt;P_\theta(x) = N_x(\mu, \Sigma)&lt;/script&gt;, where what we were calling &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is
now &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;. In this case if we define:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi_\theta(z) = \mu + \Sigma^{\frac{1}{2}}z&lt;/script&gt;

&lt;p&gt;and then sample &lt;script type=&quot;math/tex&quot;&gt;z \sim q(z)= N_z(0,I)&lt;/script&gt;, we then have that &lt;script type=&quot;math/tex&quot;&gt;p_\theta(x) = q(\phi_\theta(z))&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Once we’ve made this reparamaterisation we can return to our original problem and see that the issues have disappeared.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta E_{p_\theta(x)}[f(x)] = \nabla_\theta E_{q(z)}[f(\phi_\theta(z))] \approx \sum_i \nabla_\theta f(\phi_\theta(z))&lt;/script&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;z \sim q(z)&lt;/script&gt;. Or more concretely for our Gaussian case:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\mu,\Sigma} E_{N_x(\mu, \Sigma)}[f(x)] = \nabla_{\mu,\Sigma} E_{N_z(0,I)}[f(\mu + \Sigma^{\frac{1}{2}}z)] \approx \sum_i \nabla_{\mu,\Sigma} f(\mu + \Sigma^{\frac{1}{2}}z_i)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;z \sim N_z(0,I)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;It might at first seem that constructing these reparamaterisations might be very hard. In fact for discrete distributions, it is very
hard and in a later post we may discuss ways around this such as the Gumbel Softmax. However for continuos distributions when
you consider the fact that most random number generators first generate uniform distributed variables and
transform them, you realise that a lot of complex distributions can be built straightforwardly from simpler
distributions.&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Apr 2017 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>Intuition for Jensen's Inequality</title>
        <link>/jensen</link>
        <guid isPermaLink="true">/jensen</guid>
        <description>&lt;p&gt;I want to share some neat intuition that I came across in David Mackay’s text book
about Jensen’s inequality. Jensen’s inequality is a simple enough inequality that crops up frequently in
machine learning, especially in variational methods, when one wants to bound an
expectation.&lt;/p&gt;

&lt;p&gt;Jensen states that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[f(x)] \geq f(E[x])&lt;/script&gt;

&lt;p&gt;for any convex function f.&lt;/p&gt;

&lt;p&gt;Writing this out more verbosely it states (for discrete distributions):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum f(x_i) p_i  \geq f(\sum x_i p_i)&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;p_i = P(X = x_i)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now if we imagine that we place a mass of size &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; along the curve &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; at each of
the coordinates &lt;script type=&quot;math/tex&quot;&gt;(x_i, f(x_i))&lt;/script&gt; then &lt;script type=&quot;math/tex&quot;&gt;\sum f(x_i) p_i&lt;/script&gt; is simply the y-coordinate of the
centre of mass (remember &lt;script type=&quot;math/tex&quot;&gt;\sum p_i = 1&lt;/script&gt;). The &lt;script type=&quot;math/tex&quot;&gt;\sum x_i p_i&lt;/script&gt; is the x-coordinate.&lt;/p&gt;

&lt;p&gt;Viewed this way, all that Jensen’s inequality says is that for any convex curve, the centre of mass
of masses placed on the curve, must lie above the curve. That seems intuitively obvious to me in a way that
isn’t at first clear from staring at the inequality.&lt;/p&gt;

&lt;p&gt;The picture below shows some masses placed on a convex curve along with their centre of mass in 2-d.
It’s hopefully pretty obvious that the centre of mass of these masses has to lie above the curve (and its shown with a cross).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/jensenplot.jpg&quot; alt=&quot;my plot&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Apr 2017 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>Hello world!</title>
        <link>/hello-world</link>
        <guid isPermaLink="true">/hello-world</guid>
        <description>&lt;p&gt;Hello World!&lt;/p&gt;

&lt;p&gt;This is my first blog post. My name is Raza Habib and I’m a PhD student at UCL
studying Machine Learning under &lt;a href=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php&quot;&gt;David Barber&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’m interested in Artificial (General) Intelligence, Machine Learning, Physics, Philosophy of
Science and the study of consciousness.&lt;/p&gt;

&lt;p&gt;This blog will be a mixture of shorter technical posts explaining important ideas
within machine learning interspersed with more speculative musings about the bigger picture
questions that arise when studying computational intelligence.&lt;/p&gt;

&lt;p&gt;The layout of this blog is heavily inspired by &lt;a href=&quot;http://dustintran.com/&quot;&gt;Dustin Tran’s&lt;/a&gt; blog.&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Apr 2017 00:00:00 +0100</pubDate>
      </item>
    
  </channel>
</rss>
