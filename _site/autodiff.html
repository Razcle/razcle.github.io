<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Autodiff Can Do Your Inference For You | Raza Habib</title>
	<meta name="description" content="Continuing with the theme of neat tricks that can be very useful in Machine Learning,I wanted to share another insight that I was made aware of by Jamie Town...">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/logo.png" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="/autodiff">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Raza Habib" href="/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/">
			<img class="avatar" src="/img/photo.jpg" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/">Raza Habib</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <!-- <li>
				<a class="page-link" href="http://localhost:4000">
					about　
				</a>
			</li> -->
			
			
			
			
			<li>
				<a class="page-link" href="/about/">
					About
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			


<li>
	<a href="mailto:raza.habib@cs.ucl.ac.uk" title="Email">
		<i class="fa fa-fw fa-envelope"></i>
	</a>
</li>













<li>
	<a href="https://github.com/razcle" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>


























		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/')">
    <h1 class="title">Autodiff Can Do Your Inference For You</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="">Raza Habib</a>
        
      </span>
      <span class="date">Apr 24, 2017</span>
      <ul>
        


<li>
	<a href="mailto:raza.habib@cs.ucl.ac.uk" title="Email">
		<i class="fa fa-fw fa-envelope"></i>
	</a>
</li>













<li>
	<a href="https://github.com/razcle" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>


























      </ul>
    </div>
  </header>
  <section class="post-content"><p>Continuing with the theme of neat tricks that can be very useful in Machine Learning,
I wanted to share another insight that I was made aware of by <a href="https://github.com/j-towns">Jamie Townsend</a>
who I think may have heard it from <a href="http://people.csail.mit.edu/mattjj/">Matthew Johnson</a>. Jamie and Matthew are both contributors/authors of the Python <a href="https://github.com/HIPS/autograd">autograd</a> package
which lets you differentiate native Python. Hugely useful for prototyping.</p>

<p>The trick that Jamie pointed out to me is that for exponential family models, because the expected sufficient statistics
are related straightforwardly to the derivative of the log-normaliser, you can do a lot of your inference with no extra
effort by just leveraging automatic differentiation. I’ll very briefly recap the exponential family and the connection of derivatives to sufficient statistics before giving a quick example of just how easy this can be.</p>

<h3 id="exponential-family-distributions">Exponential Family distributions</h3>
<p>An exponential family distribution is just one that can be written in the following form:</p>

<script type="math/tex; mode=display">P(x|\theta) = g(\eta(\theta))f(x)e^{\eta(\theta)^T T(x)}</script>

<p>Where <script type="math/tex">\theta</script> are the standard parameters of the distribution, <script type="math/tex">\eta(\theta)</script> is a function of those parameters and is known as
the natural parameters and <script type="math/tex">T(x)</script> are the sufficient statistics. <script type="math/tex">g(\theta)</script> is simply the reciprocal of the normaliser.  Many if not most of the standard distributions we deal with can be written in this way eg. Normal, Poisson, Exponential, Laplacian, Bernoulli, Beta…</p>

<h3 id="the-derivatives-of-log-normaliser-are-the-expected-sufficient-stats">The Derivatives of Log-normaliser are the Expected Sufficient Stats</h3>

<p>There is a well known identity that says for the exponential family:</p>

<script type="math/tex; mode=display">-\nabla_{\eta} log(g(\eta)) = E_{p(x|\theta)}[T(x)]</script>

<p>which is fairly straightforward to show as follows:</p>

<script type="math/tex; mode=display">-\nabla_{\eta} log(g(\eta)) = g(\eta)\nabla_{\eta}\frac{1}{g(\eta)}</script>

<p>and we know that <script type="math/tex">g(\eta)</script> is the inverse of the normaliser:</p>

<script type="math/tex; mode=display">\frac{1}{g(\eta)} = \int f(x)e^{\eta(\theta)^T T(x)} dx</script>

<p>and so:</p>

<script type="math/tex; mode=display">\nabla_{\eta}\frac{1}{g(\eta)} = \int T(x) f(x)e^{\eta(\theta)^T T(x)} dx</script>

<p>Thus:</p>

<script type="math/tex; mode=display">-\nabla_{\eta} log(g(\eta)) = g(\eta)\nabla_{\eta}\frac{1}{g(\eta)} =
\int T(x) g(\eta) f(x)e^{\eta(\theta)^T T(x)} dx = E_{p(x|\theta)}[T(x)]</script>

<h3 id="using-auto-diff-to-do-your-inference">Using auto-diff to do your inference</h3>

<p>Many times when doing inference of graphical models the expected sufficient statistics
are actually exactly what we want. For example when we do the EM algorithm over jointly exponential family
models, the E-step only requires us to find the expectations of the sufficient statistics under the posterior on the latents.</p>

<p>In the <a href="https://github.com/HIPS/autograd/blob/master/examples/hmm_em.py">autograd</a> library, there is a nice example of taking advantage of this fact to do inference in an HMM.
Traditionally you would do your inference in an HMM using the forward-backward or Baum-Welch algorithm but the autograd library takes
advantage of the exponential family structure to do all the inference using differentiation.</p>

<p>For an HMM with observed variables <script type="math/tex">y_{1:T}</script>, discrete latents <script type="math/tex">x_{1:T}</script> and
transition probabilities <script type="math/tex">p(y_t=j|x_t=i)=B_{ij}</script>,
 <script type="math/tex">p(x_t|x_{t-1})=A_{ij}</script>. The joint probability over all the <script type="math/tex">\mathcal{X}</script> and <script type="math/tex">\mathcal{Y}</script> is given by:</p>

<script type="math/tex; mode=display">logP(\mathcal{X},\mathcal{Y}) = \sum_i x_0^i log \pi_i + \sum_t \sum_{i,j}
x_t^i x_{t+1}^j log A_{ij} + x_t^i y_t^j log B_{ij}</script>

<p>where we have encoded the <script type="math/tex">x</script> and <script type="math/tex">y</script> using one-hot vectors. This is clearly in the exponential family with sufficient statistics <script type="math/tex">\sum_t x_t^T x_{t+1}</script>, <script type="math/tex">\sum_t x_t^T y_{t}</script> and <script type="math/tex">x_0</script>.</p>

<p>To do inference what we need is the expectation of these statistics under
<script type="math/tex">P(\mathcal{X}|\mathcal{Y})</script>. We can find <script type="math/tex">P(\mathcal{X}|\mathcal{Y})</script> by simply summing the joint over
<script type="math/tex">\mathcal{Y}</script>. In fact all we really need is the log normaliser <script type="math/tex">Z = log \sum_y P(\mathcal{Y},\mathcal{X})</script>.
Once we have the normaliser we can take its derivative and this will automatically take care of all our message passing for us.</p>

<p>Thats exactly what the autgrad libarary does in the following example:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># First get the log partition function</span>
<span class="k">def</span> <span class="nf">log_partition_function</span><span class="p">(</span><span class="n">natural_params</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">log_partition_function</span><span class="p">,</span> <span class="n">natural_params</span><span class="p">),</span> <span class="n">data</span><span class="p">))</span>

<span class="n">log_pi</span><span class="p">,</span> <span class="n">log_A</span><span class="p">,</span> <span class="n">log_B</span> <span class="o">=</span> <span class="n">natural_params</span>

<span class="n">log_alpha</span> <span class="o">=</span> <span class="n">log_pi</span>
<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">log_alpha</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_alpha</span><span class="p">[:,</span><span class="bp">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">log_A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_B</span><span class="p">[:,</span><span class="n">y</span><span class="p">]</span>

<span class="k">return</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_alpha</span><span class="p">)</span>
</code></pre>
</div>
<p>and then you can do all the inference simply using gradients:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">EM_update</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
       <span class="n">natural_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
       <span class="c"># E step</span>
       <span class="n">loglike</span><span class="p">,</span> <span class="n">E_stats</span> <span class="o">=</span> <span class="n">val_and_grad</span><span class="p">(</span><span class="n">log_partition_function</span><span class="p">)(</span><span class="n">natural_params</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
       <span class="k">if</span> <span class="n">callback</span><span class="p">:</span> <span class="n">callback</span><span class="p">(</span><span class="n">loglike</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
       <span class="c"># M step</span>
       <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">normalize</span><span class="p">,</span> <span class="n">E_stats</span><span class="p">))</span>
</code></pre>
</div>

<h3 id="why-does-this-work">Why does this work?</h3>

<p>At first it might seem a bit mysterious that we can replace a forwards and backwards
message passing algorithm by auto-diff alone but the reason this works is because
the sum product algorithm and reverse mode auto-diff are in fact very similar. Reverse mode
auto-diff first does a forward pass to calculate the value of the function and then retraces the computation graph
multiplying gradients along branches and summing at mergers in a backwards pass. This is highly analogous to the standard
forward backwards algorithm.</p>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="https://github.com/Razcle">Raza</a>
</p>
</footer>


  </body>
</html>
