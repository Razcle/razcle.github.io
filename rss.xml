<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <link>/</link>
    <description></description>
    <pubDate>Sun, 20 May 2018 16:31:55 +0100</pubDate>
    
      <item>
        <title>Adaptive MCMC - A cautionary example</title>
        <link>/adaptiveMCMC</link>
        <guid isPermaLink="true">/adaptiveMCMC</guid>
        <description>&lt;p&gt;The goal of adaptive Markov Chain Monte Carlo is to develop 
more efficient samplers by adapting the Markov transition operator during
sampling.&lt;/p&gt;

&lt;p&gt;An easy mistake to make is to think that as long as during adaptation the Markov
operator always has the correct stationary distribution then the overall sampler will be correct
but this is not true. Here’s a simple intuitive example that shows that this is not correct.
Imagine that we wish to sample from a Bernoulli distribution  where each state has an equal probability of 
being sampled. We could doing this by simulating from the Markov chain shown in the picture below. The arrows 
represent possible transitions and their probabilities.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/adaptivemcmc.png&quot; alt=&quot;my plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s clear by symmetry that if we start in state 1 and hop along the arrows according to the
probabilities shown, that we’ll spend an equal amount of time in both states no matter what the value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is. So following
this Markov chain would allow us to simulate from a fair coin and we could change the starting value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and 
have no affect.&lt;/p&gt;

&lt;p&gt;What would happen however, if we started simulating this Markov chain and then changed the value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.
Well initially both states 1 and 2 would be equally likely by symmetry but when I change &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, lets say 
made it bigger, then the state the chain is in when &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; becomes bigger
will now be less likely than the other state. Overall, our chain will no longer
be simulating from a fair coin. This is despite the fact that if we first fixed &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;
to any value and then simulated with that fixed &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, we’d always simulate a fair coin.&lt;/p&gt;

&lt;p&gt;This shows that care must be taken when adapting transitions during MCMC. For most chains
we can guarantee that adaptation wont break our sampler as long as the amount of adaptation tends to &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; as 
the length of the chain tends to infinity. This criterion is known as “vanishing adaptation”. In general
we should use adaptation with caution.&lt;/p&gt;
</description>
        <pubDate>Sun, 20 May 2018 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>How bad can importance sampling be?</title>
        <link>/importancesampling</link>
        <guid isPermaLink="true">/importancesampling</guid>
        <description>&lt;p&gt;How bad can importance sampling be? Importance sampling is regularly used to estimate
expectations or simply to reduce variance. In RL its also used to perform off-policy learning.
If the approximating distribution used is poorly chosen, importance sampling
can actually increase variance. In fact it’s known that the variance of the importance weights
can become unbounded. Despite this being common knowledge it’s easy to be lulled into false
conclusions when using importance sampling. Here’s an incredibly simple 1-d example that
shows how badly wrong things can go:&lt;/p&gt;

&lt;p&gt;Imagine that our target distribution, &lt;script type=&quot;math/tex&quot;&gt;P(x)&lt;/script&gt; is a zero mean 1-dimensional gaussian with unkown
variance &lt;script type=&quot;math/tex&quot;&gt;\sigma_p&lt;/script&gt; and that our proposal distribution is ALSO a 1-dimensional zero-mean
gaussian but with the wrong variance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/importancesampling1.png&quot; alt=&quot;setup&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This setting might seem like an incredibly benign one
for importance sampling - the target and approximating distributions are well matched and
the dimensionality is low! However, lets consider the variance of the importance weights:
&lt;script type=&quot;math/tex&quot;&gt;W = \frac{P(x)}{Q(x)}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_Q(W) = 1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_Q(W^2) = \int dx \left(\frac{2\pi \sigma_q^2}{2 \pi \sigma_p^2} \right)} \frac{1}{\sqrt{2\pi \sigma_q^2}} e^{  -(\frac{1}{\sigma_p^2} - \frac{1}{\sigma_q^2})x^2 - \frac{x^2}{2\sigma_q^2}&lt;/script&gt;

&lt;p&gt;After some manipulation this gives that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Var(W) = \frac{\sigma_q^2}{\sigma_p \sqrt{2\sigma_q^2 - \sigma_p^2}}  - 1&lt;/script&gt;

&lt;p&gt;which diverges as &lt;script type=&quot;math/tex&quot;&gt;\sigma_q \rightarrow \frac{\sigma_p}{\sqrt{2}}&lt;/script&gt;!&lt;/p&gt;

&lt;p&gt;However, there would be nothing to stop us performing importance sampling in this case.
What happens in practice if we try to estimate the normalisation of &lt;script type=&quot;math/tex&quot;&gt;P(x)&lt;/script&gt; (which we know to be one)
when &lt;script type=&quot;math/tex&quot;&gt;\sigma_q&lt;/script&gt; is small enough for the variance to diverge?&lt;/p&gt;

&lt;p&gt;Below is the estimate for the normalisation along with it’s standard deviation for &lt;script type=&quot;math/tex&quot;&gt;\sigma_p=1&lt;/script&gt;
and &lt;script type=&quot;math/tex&quot;&gt;\sigma_q&lt;/script&gt; varied between &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/mean_estimate.png&quot; alt=&quot;mean&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/std_estimate.png&quot; alt=&quot;std&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see that when &lt;script type=&quot;math/tex&quot;&gt;\sigma_q&lt;/script&gt; drops below 0.7 the estimate of the normalisation
is totally wrong but the empirical standard deviation continues to be quite small even
when the true variance is infinite!&lt;/p&gt;

&lt;p&gt;In this extremely simple case, we can analytically calculate the variance and see that
our importance sampling estimates are bogus. Despite this, our empirical estimate of the
standard deviation might lead us to believe that our calculations are fine. This type of
problem is much harder to diagnose in realistic settings and I dont really have an answer
as to how to avoid this in practice.&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      </item>
    
      <item>
        <title>What do I know if I'm ignorant? - Mackay's Needle</title>
        <link>/mackays-needle</link>
        <guid isPermaLink="true">/mackays-needle</guid>
        <description>&lt;p&gt;Came across another wonderful bit of wisdom from the late great Professor Sir
David Mackay when going back through his text book, &lt;em&gt;Information Theory, Inference
and Learning Algorithms&lt;/em&gt;. This is probably one of my favourite books and I keep
picking up on a little nuances every time I read it.&lt;/p&gt;

&lt;p&gt;The chapter I was reading this time is “Random Inference Topics” and the section
is titled “What do I know if I’m ignorant”. The big takeaway of this section
is to remind us that ignorance doesn’t necessarily imply uniform distributions over
possible outcomes. Mackay illustrates this with a few simple puzzles and I particularly
enjoyed one that I’m calling “Mackay’s Needle”.&lt;/p&gt;

&lt;h2 id=&quot;mackays-needle&quot;&gt;Mackay’s Needle&lt;/h2&gt;

&lt;p&gt;There’s a famous probability puzzle knows and Buffon’s needle which asks the following:
“If I drop a needle onto a stripy floor, what’s the probability that the needle lands
at the intersection between two stripes?”&lt;/p&gt;

&lt;p&gt;“Mackay’s needle” takes a similar flavour and asks the following:&lt;/p&gt;

&lt;p&gt;“A pin is thrown tumbling in the air. What is the probability distribution of the angle &lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt; between the pin and the vertical at a moment while it is in the air? The tumbling pin is photographed. What is the probability distribution of the angle &lt;script type=&quot;math/tex&quot;&gt;\theta_3&lt;/script&gt; between the pin and the vertical as imaged in the photograph?”&lt;/p&gt;

&lt;h1 id=&quot;spoiler-alert&quot;&gt;Spoiler ALert&lt;/h1&gt;
&lt;p&gt;The problem is fairly straightforward and if you’d like to solve it yourself then
I’d advise pausing here.&lt;/p&gt;

&lt;h3 id=&quot;my-solution&quot;&gt;My solution&lt;/h3&gt;

&lt;p&gt;A naive first response to this problem is to say that since we know nothing about
the position of the needle, all angles from the vertical are equally likely. i.e
we might naively say that &lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt; takes a uniform distribution between &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and
&lt;script type=&quot;math/tex&quot;&gt;\pi/2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If you think about the problem for a little longer you’ll notice that actually
it isn’t quite that simple. For any value of &lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt; we can rotate the needle
around the vertical axis by any angle &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; and we still wont change the value of &lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt;.
For larger values of &lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt; the circle traced out by the tip of the needle as we rotate it
is bigger, so there are in some sense more ways to achieve larger values of &lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can make this more concrete by calculating the density, &lt;script type=&quot;math/tex&quot;&gt;P(\theta_1)&lt;/script&gt;.
We know that the density will be proportional to the size of the circle traced
out by the tip of the needle so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta_1)d\theta_1 \propto \pi r sin(\theta_1) d\theta_1 \propto sin(\theta_1)d\theta_1&lt;/script&gt;

&lt;p&gt;Where r is the distance of the centre of the needle to its tip.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;sin&lt;/script&gt; integrates to 1 in the range &lt;script type=&quot;math/tex&quot;&gt;[0, \pi/2]&lt;/script&gt; this is already normalised and so
we have our denstity &lt;script type=&quot;math/tex&quot;&gt;P(\theta_1) = sin(\theta_1)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In the case that we suppress one dimension (like the photograph described above),
we actually do return to a uniform distribution over &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Feb 2018 00:00:00 +0000</pubDate>
      </item>
    
      <item>
        <title>DeepMind at UCL - Academia meets Industry</title>
        <link>/DeepMindAtUCL</link>
        <guid isPermaLink="true">/DeepMindAtUCL</guid>
        <description>&lt;p&gt;This term I’m the teaching assistant (TA) on the deep-mind lead course,
“Advanced Deep Learning and Reinforcement Learning” at UCL, and this has prompted
me to think a little about DeepMind’s approach to AI and the impact of industry/academia
partnerships, so I decided to jot down a few of my thoughts.
I should caveat that I’m not employed by DeepMind (I’m employed by UCL) and everything I say is simply
my interpretation of publicly available information.&lt;/p&gt;

&lt;p&gt;The first thing that I wanted to discuss was how DeepMind describe their approach
to solving AGI. The first lecture in the course was focussed primarily on this question
both from a theoretical perspective, how do we define what it means to “solve intelligence”, and
a practical perspective, how do you organise a large number of academics to work in teams on the
same problem. I find both of these questions fascinating, especially the question of how to manage
scientific progress, something that I think is typically overlooked in Academia.&lt;/p&gt;

&lt;p&gt;In this post I’m going to focus on the question on what DeepMind mean by “solving intelligence”, mainly because
they’re more public on their thinking in this area. If I have time, I’ll try to share my thoughts on what I’ve learned
about managing large scale science.&lt;/p&gt;

&lt;h2 id=&quot;deepminds-stated-approach-to-solving-intelligence&quot;&gt;DeepMind’s Stated Approach to “Solving Intelligence”&lt;/h2&gt;

&lt;p&gt;DeepMind’s mission statement is as audacious as it is succinct - “solve intelligence, use it to make the world a better place”. It’s the kind of statement that sometimes frustrates serious academics for being overly vague and possibly exaggerating the state of modern AI. However, I quite like the statement.
I find it inspiring that they’re willing to set ambitious goals even if they’re a long way off and DeepMind seems to have well thought out answers
to what it means to solve intelligence.&lt;/p&gt;

&lt;h3 id=&quot;what-is-intelligence&quot;&gt;What is Intelligence?&lt;/h3&gt;

&lt;p&gt;DeepMind’s answers to both the questions of what intelligence is and how to solve it, have their roots in the PhD work of their co-founder and chief scientist Shane Legg.
Legg did his PhD, titled &lt;a href=&quot;http://www.vetta.org/documents/Machine_Super_Intelligence.pdf&quot;&gt;“Machine Superintelligence”&lt;/a&gt;, with Marcus Hutter who is famously theoretical and intellectually ambitious.
Hutter is mostly famous for working on “Universal AI”, and anyone who’s ever been to a talk by Juergen Schmidhuber will have heard about
Hutter’s asymptotically optimal algorithm for searching the space of all computable programs. (Sadly not computable in practice)&lt;/p&gt;

&lt;p&gt;During his PhD, Legg grew frustrated by the lack of a clearly defined goal for AGI research. Lots of people
say they are working in AI (or other people say it for them) but they have very different goals. Without a clearly
defined target its difficult to measure progress and the goal posts keep moving. Hutter and Legg studied various definitions
of intelligence from a range of different disciplines and presented both a mathematical formalisation and a colloquial definition
of what intelligence is.&lt;/p&gt;

&lt;p&gt;Colloquially, Legg says “Intelligence measures an agent’s ability to achieve goals in a wide range of
environments.”&lt;/p&gt;

&lt;p&gt;This definition is remarkably simple and hides a huge amount of thought behind it. Hutter and Legg
collected dozens of alternative definitions of intelligence from experts in various fields and this definition somehow manages
to capture almost all of them. Importantly this definition doesn’t distinguish whether the agent is biological, a human, an animal
or a silicon based machine. All agents, biological and artificial can be judged under this criterion and measured.&lt;/p&gt;

&lt;p&gt;Formally they define an agent’s “intelligence” to be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Upsilon(\pi) = \sum_{\mu \in E} 2^{-K(\mu)} V^\pi_\mu&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is an agent or policy, &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; is the space of all “computable reward summable environment measures”,  &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; is the
Kolmogorov complexity of the given environment and &lt;script type=&quot;math/tex&quot;&gt;V^\pi_\mu&lt;/script&gt; is the value achieved by that agent in that environment. This definition captures
the colloquial one given above. Interestingly more complex environments are actually penalised relative to simple ones, this means that advanced but
highly specialised agents (like a chess computer) would still score poorly on this universal intelligence measure.&lt;/p&gt;

&lt;p&gt;In practice the Kolmogorov complexity cant be calculated so this definition is more of a mathematical curiosity then a practical definition.
None the less, this formal work guides the strategy at DeepMind. They build a wide range of environments of increasing complexity and can measure
progress by how much reward a single agent can achieve across many environments.&lt;/p&gt;

&lt;p&gt;To date the results are mixed but having a concrete and well defined goal make DeepMind particular and quite unusual amongst research labs.&lt;/p&gt;

&lt;h3 id=&quot;grounded-cognition-and-rl-or-how-do-you-solve-it&quot;&gt;Grounded Cognition and RL or How do you solve it?&lt;/h3&gt;

&lt;p&gt;Having answered the question of what they mean by solving intelligence DeepMind then go further
and present a strategy or plan of attack. One of the key ideas behind DeepMind’s approach is that of
&lt;em&gt;grounded cognition&lt;/em&gt;. AI agents, they say, have to be immersed in a rich sensorimotor experience and derive their
behaviour directly from that experience. The reason for grounded cognition being so
important is that it helps avoid some of the short comings of more traditional non-learning based AI
algorithms. Simplifying enormously, a more traditional symbolic AI system requires a human to find the correspondence/isomorphism
between symbols in the system and objects in the world. This means that setting up these systems requires a lot of
domain expertise. If on the other hand, an agent can extract information directly from sensory input then
it can operate in “the wild” more easily and can adapt naturally to different environments.&lt;/p&gt;

&lt;p&gt;The need for grounded cognition is part of the reason why deep learning is such a focus for DeepMind.
The main strength of deep learning in this regard is that it can directly extract useful features from
data. It could be argued that non-parametric methods like Support Vector Machines
or Gaussian Processes can also perform feature extraction/selection and this is true but it’s much harder
to reuse the (infinite dimensional) learned features and some of these methods don’t scale well to large data sets. There are groups pursuing this approach though, eg &lt;a href=&quot;https://www.prowler.io/&quot;&gt;prowler&lt;/a&gt; and they have produced amazing work as well.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;They’re are mixed feelings within academia about industrial research labs and
DeepMind in particular. Many argue that they over do their PR and receive disproportionate attention
for ideas that are maybe less novel then they suggest. There are also concerns of a brain drain from
academia and an inability of academics to compete with the vast data and compute resources of Google.
Personally, I think that the benefits far outweigh the cons. DeepMind have more clearly defined what it
means to solve AI then any other group, and by articulating a well defined vision they’ve inspired
many more people to take an interest in these problems. Academia will continue to be an important source
of major contributions but I personally welcome the competition form industry and the heavy computational
lifting that they can do on the community’s behalf.&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      </item>
    
      <item>
        <title>Scaling Ambition - 10 bits of advice from Reid Hoffman</title>
        <link>/scalingambition</link>
        <guid isPermaLink="true">/scalingambition</guid>
        <description>&lt;h3 id=&quot;starting-a-company-is-like-jumping-off-a-cliff-and-assembling-a-plane-on-the-way-down--reid-hoffman-2018&quot;&gt;“&lt;em&gt;Starting a company is like jumping off a cliff and assembling a plane on the way down&lt;/em&gt;” – Reid Hoffman, 2018&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/hoffman.jpg&quot; alt=&quot;hoffman&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yesterday evening I had the pleasure of seeing Reid Hoffman discuss
entrepreneurship in a talk organised by &lt;a href=&quot;https://joinef.com&quot;&gt;Entrepreneur First&lt;/a&gt;.
I’ve been lucky enough to visit a few of these events and I thought this one
was exceptionally good. The videos have been made available on line &lt;a href=&quot;https://www.youtube.com/watch?v=WqVL3CBN560&quot;&gt;here&lt;/a&gt;.
If you don’t have time to see the whole talk, below is a summary of some of my favourite
bits of advice given. Most of the advice is aimed at entrepreneurs but much of it should
transfer over to anyone ambitious, trying to maximise their impact.&lt;/p&gt;

&lt;h3 id=&quot;1-be-sure-youre-learning&quot;&gt;1) Be sure you’re learning&lt;/h3&gt;
&lt;p&gt;This piece of advice came up in a few different places throughout the talk. People
often discuss the value of failure because of its potential for learning but
how can you be sure you’re actually learning from your experiences? The suggested
tip here is to imagine a relatively recent slice of time - a few weeks or months -
and ask yourself what advice you would give your younger self. If you cant think
of any advice or what you would do differently, then
you’re probably not learning.&lt;/p&gt;

&lt;h3 id=&quot;2-its-hard-to-overstate-the-importance-of-grit&quot;&gt;2) It’s hard to overstate the importance of grit&lt;/h3&gt;
&lt;p&gt;When asked about some of the reasons for Silicon Valley’s success, Reid suggested
that a big component was attitudes to failure. By not demonising failure and allowing
people multiple chances, Silicon Valley de-risks starting a business
and allows the most gritty individuals to shine.&lt;/p&gt;

&lt;p&gt;Roughly speaking, grit is a measure of an individual’s ability to stay focused on
a given project, persevere through difficulties and maintain enthusiasm for their
goal. Gritty individuals tend to connect their purpose to other people, put in
hours of deliberate practice and view failure as an opportunity for learning. You
can self assess your own grit by calculating your grit score &lt;a href=&quot;https://angeladuckworth.com/grit-scale/&quot;&gt;here&lt;/a&gt;.
Many believe that grit itself can be developed and improved.&lt;/p&gt;

&lt;p&gt;Another interesting data point that came up with regards to grit is that amongst the
Entrepreneur First cohorts, some of the most successful companies were started by
people who went through the program, failed and then started again. One possible
reason for the greater success amongst the subset of failure is a demonstration
of exceptional grit.&lt;/p&gt;

&lt;h3 id=&quot;3-the-best-technical-founders-are-also-business-model-geeks&quot;&gt;3) The best technical founders are also business model geeks&lt;/h3&gt;
&lt;p&gt;When considering some of the tech behemoths like Google, Facebook or Microsoft;
it’s easy for engineering-types to over emphasise the technical skills of the founders.
Sergei, Larry, Gates and Zuckerberg all have serious technical credentials but one thing
that remains super important is that they were also business model geeks.&lt;/p&gt;

&lt;p&gt;That doesn’t mean they’d absorbed the business school knowledge of an MBA but
rather that they were willing to think creatively about how to monetise their
products.&lt;/p&gt;

&lt;p&gt;Some example strategies that Reid implicitly mentioned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Freemium models - providing a core product for free and charging for extra services. Examples here
might be LinkedIn or Spotify.&lt;/li&gt;
  &lt;li&gt;Trojan horse - provide a lower quality and cheaper version of a product than competitors
to gain access to some other valuable commodity like a user base, data or network. Examples
here might be Facebook or Onfido.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Leading directly from this point is the idea that you shouldn’t be too restricted
by conventional norms or rules:&lt;/p&gt;

&lt;h3 id=&quot;4-find-where-you-can-break-traditional-rules&quot;&gt;4) Find where you can break traditional rules&lt;/h3&gt;
&lt;p&gt;One of things that the most successful founders do, is to think carefully about
where they may be able to ignore conventional wisdom and give themselves greater
bandwidth for their most important problems. Most of us, but entrepreneurs to a greater
degree, are usually fighting battles on numerous fronts and need to ruthlessly
prioritise. Sometimes its easy to feel like you have to do something because
“thats the way that its done”, so its important to
question that feeling.&lt;/p&gt;

&lt;p&gt;A concrete example Reid gave was the role of unit economics in start-ups. Traditional
business wisdom would say that you should start calculating unit economics very early,
you should know the life-time-value of your customers etc. However for software businesses
in Silicon Valley it’s fairly common to see series-A or even series-B start-ups that still
haven’t answered this question because it’s not yet a priority. Their founders have realised
their efforts are better spent else where. That isn’t to say that knowing your unit economics
isn’t important, a hardware business would die if they didn’t know this, but you shouldn’t
be bound into this rule just because its conventional.&lt;/p&gt;

&lt;h3 id=&quot;5-engage-in-abz-planning&quot;&gt;5) Engage in ABZ planning&lt;/h3&gt;
&lt;p&gt;It’s pretty typical for people to have a Plan A, and a Plan B, and maybe a Plan C.
Reid Hoffman suggested this was the wrong way to go about this. Rather you should
have a plan A, many Plan&lt;em&gt;s&lt;/em&gt; B and then if all else fails a plan Z. The key idea
here is that if plan A doesn’t work, it might not work in lots of different ways
and you should have multiple strategies for still achieving your goal depending
on the outcome of plan A. Then if all of these fail, you should have a backup:
your plan Z. Plan Z is your parachute, it should give you a floor to how bad things
can get and therefore allow you to take more risk.&lt;/p&gt;

&lt;h3 id=&quot;6-the-best-businesses-and-teams-have-short-ooda-loops&quot;&gt;6) The best businesses and teams have short OODA loops&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/OODA_loop&quot;&gt;OODA loop&lt;/a&gt; is a term from the United
States Airforce that stands for Observe, Orient, Decide, Act. When asked for
unifying traits of the most successful teams, fast OODA loop was amongst the
first things that Reid mentioned.&lt;/p&gt;

&lt;p&gt;As far as I can tell OODA loop is akin to the “scientific method”*  but with an
extra O for orientation. Orientation is the process by which we filter our observations
through the various lenses of our past experience, knowledge and culture.&lt;/p&gt;

&lt;p&gt;I can’t claim a deep understanding of the concepts behind OODA-loop but I think
the main point that Reid was making, was that the best teams are decisive
but also data-driven.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/OODA.png&quot; alt=&quot;ooda&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;7-be-very-intentional-about-culture&quot;&gt;7) Be very intentional about culture&lt;/h3&gt;
&lt;p&gt;When reflecting on his own experience as an entrepreneur and of LinkedIn, one of
the things Reid said he would have done differently is culture. He suggested
that trying to be explicit about culture from very early on and taking an active
hand in shaping it were super important.&lt;/p&gt;

&lt;p&gt;He gave two concrete examples:&lt;/p&gt;

&lt;p&gt;1) &lt;a href=&quot;https://www.slideshare.net/reed2001/culture-1798664/37-Why_are_we_so_insistent&quot;&gt;The Netflix culture document&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) The founder of Workday, Aneel Bhusri, interviewed the first 500 employees
and on every rejection advised senior management on why he had rejected that
candidate. In this way he both demonstrated the importance of culture and was explicitly
able to communicate what we wanted to other senior management.&lt;/p&gt;

&lt;h3 id=&quot;8-references-are-more-important-than-interviews&quot;&gt;8) References are more important than interviews&lt;/h3&gt;
&lt;p&gt;When it comes to choosing who to hire, you can only learn so much through interviews
and ultimately the interview can be gamed.&lt;/p&gt;

&lt;p&gt;Reid suggested going through the following loop when hiring for key roles:&lt;/p&gt;

&lt;p&gt;1) First reach out to your network and ask people who is the best person they know
at a given job, not the best person available but the best person full stop.&lt;/p&gt;

&lt;p&gt;2) Approach this person and try to learn as much about the job from them as is possible.
Then ask them to recommend somebody.&lt;/p&gt;

&lt;p&gt;3) Once you have a candidate in mind, get 5-7 different people to provide references.
People tend not to want to say negative things in references so ask for ratings on a
scale of 1-10. Mostly 7’s is a rejection but mostly 8-9’s is very interesting.&lt;/p&gt;

&lt;h3 id=&quot;9-correct-your-mistakes-early---if-there-is-a-question-there-is-no-question&quot;&gt;9) Correct your mistakes early - “if there is a question, there is no question”&lt;/h3&gt;

&lt;p&gt;This was brought up specifically in the context of hiring but is probably applicable to
any decision with large long term consequences. Its very important to correct mistakes early
and if you have significant doubts about a hire, than you should probably fire them. As the
title says, “if there is a question, there is no question”.&lt;/p&gt;

&lt;h3 id=&quot;10-rely-very-strongly-on-your-network&quot;&gt;10) Rely very strongly on your network&lt;/h3&gt;

&lt;p&gt;The concept of having a good network of friends and contacts that can help you
came up numerous times across the evening and in lots of different contexts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to validate an early business model?:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“take your hypothesis to the smartest people you know and seek critical feedback”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How can a large company stay innovative?:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“maintain a connection to the start-up community”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to find good employees?:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Ask your network to recommend the best person they can?”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to overcome market scepticism?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Leverage outside brand, such as a strong VC”&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Ultimately, if you want to get the full value of the evening, you’ll have to watch
the original video as there were lots of little nuggets distributed throughout the talk.
I’ve inevitably filtered the advice through my own lens but hopefully the above
list gives you a taste of what was said and will prove helpful.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p&gt;*I’m not sure I subscribe to the convential view of the scientific method but that
will have to wait for a later post.&lt;/p&gt;
</description>
        <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
      </item>
    
      <item>
        <title>Learning to Sample - part 1</title>
        <link>/SamplingInML</link>
        <guid isPermaLink="true">/SamplingInML</guid>
        <description>&lt;p&gt;I’ve become really interested in strategies for sampling from intractable
probability distributions, especially MCMC. Not only does sampling crop up all over machine learning
but I also find the problem intrinsically intellectually
appealing. Sampling is  &lt;em&gt;deceptively&lt;/em&gt; difficult -
a very easy problem to state but hugely complex to solve.&lt;/p&gt;

&lt;p&gt;Recently my attention has been caught by MCMC strategies that involve
learning how to sample. There have been a couple of papers in this vein:
&lt;em&gt;Generalising Hamiltonain Monte Carlo with Neural Nets&lt;/em&gt; and &lt;em&gt;A-NICE-MC&lt;/em&gt;
are both excellent examples.&lt;/p&gt;

&lt;p&gt;In this  post I will:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Describe the problem MCMC is trying to solve.&lt;/li&gt;
  &lt;li&gt;Recap the basics of Metropolis Hastings.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next post:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Delve into some of the cutting edge strategies for learning samplers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-problem&quot;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;Concretely the problem that frequently crops up in ML is either:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Draw samples &lt;script type=&quot;math/tex&quot;&gt;{x}&lt;/script&gt; from a probability distribution &lt;script type=&quot;math/tex&quot;&gt;P(x)&lt;/script&gt; or&lt;/li&gt;
  &lt;li&gt;Compute the expectation &lt;script type=&quot;math/tex&quot;&gt;E_{p(x)}[f(x)]&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where the distribution &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; is very complicated. Often we only know &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;
up to some normalisation constant &lt;script type=&quot;math/tex&quot;&gt;p(x)=p^*(x)/Z&lt;/script&gt; and we can only evaluate &lt;script type=&quot;math/tex&quot;&gt;p^*(x)&lt;/script&gt;
at a point.&lt;/p&gt;

&lt;p&gt;This problem comes up in inference when marginalising over variables, learning with intractable normalisers
and in model comparison when computing the model evidence.&lt;/p&gt;

&lt;h3 id=&quot;one-concrete-example---bayesian-deep-learning&quot;&gt;One Concrete Example - Bayesian Deep learning&lt;/h3&gt;

&lt;p&gt;I find it helps to have a motivating example in mind. One cool example of MCMC
that bridges the worlds of traditional probabilistic
learning and Deep Learning is sampling the posterior over the weights of a neural network.
I think Radford Neal was one of the first to notice the opportunity here and applied an
MCMC algorithm called Hamiltonian Montecarlo to the problem and got excellent results.&lt;/p&gt;

&lt;p&gt;Imagine your using a neural net for a regression task. Given an input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; your neural
net outputs a prediction &lt;script type=&quot;math/tex&quot;&gt;f_W(x)&lt;/script&gt;. Typically neural nets are trained by optimising a cost
function, maybe with some kind of regulariser:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L} = \sum_n \frac{1}{2} \left(y_n - f_W(x_n) \right)^T \left(y_n - f_W(x_n) \right) + \frac{1}{2} W^TW&lt;/script&gt;

&lt;p&gt;Another equivalent view, is that this cost function corresponds to MAP inference in the following
probabilistic model. The likelihood is just a gaussian whose mean is &lt;script type=&quot;math/tex&quot;&gt;f_W(x)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y|x, W) = |2\pi I|\ e^{- \frac{1}{2} \left(y - f_W(x) \right)^T \left(y - f_W(x) \right) }&lt;/script&gt;

&lt;p&gt;and the prior is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(W) = (2 \pi)^{- \frac{D}{2}} e^{- \frac{1}{2} W^TW}&lt;/script&gt;

&lt;p&gt;and the data are assumed to be drawn i.i.d. In this case our cost function &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}&lt;/script&gt;
is equivalent to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L} = - \sum_n \log P(y_n|x_n, W) - \log P(W)&lt;/script&gt;

&lt;p&gt;Rather than doing MAP inference in this model we could actually draw samples from
the posterior over model parameters, &lt;script type=&quot;math/tex&quot;&gt;P(W| \{x_1, ..., x_N\}, \{y_1, ..., y_n\})&lt;/script&gt;.
If we do this, not only will our model predictions likely be more accurate than just taking
the MAP but we can also calculate uncertainty estimates for all of our predictions.
Our predictions now become:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^* | x^* = E_{P(y|x^*, W)}[y] \approx \sum_m^M \frac{1}{M} f_{W_m}(x)&lt;/script&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_m \sim P(W| \{x_1, ..., x_N\}, \{y_1, ..., y_n\})&lt;/script&gt;

&lt;p&gt;and our uncertainty becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_y = Var(y|x) \approx \sum_m \frac{1}{M} f_{W_m}(x)f_{W_m}(x)^T - y^*y^{*T}&lt;/script&gt;

&lt;p&gt;Being able to estimate uncertainty is incredibly important when deploying deep learning
in the real world. For example when used in medical diagnosis, we may not want to trust
our deep learning model when the uncertainty is high.&lt;/p&gt;

&lt;p&gt;In order to get these uncertainty estimates we need to able to sample from &lt;script type=&quot;math/tex&quot;&gt;P(W| \{x_1, ..., x_N\}, \{y_1, ..., y_n\})&lt;/script&gt;.
This is far from trivial.&lt;/p&gt;

&lt;h3 id=&quot;what-makes-this-so-hard&quot;&gt;What makes this so hard?&lt;/h3&gt;
&lt;p&gt;Sampling requires global information but if we can only evaluate the density point-wise, our information
is inherently local.
By definition, to be able to draw samples from a distribution we need samples on
average to come from places that contain a large fraction of the total
probability mass. However, knowing the fraction of probability mass in a given region
is an inherently global property. We need to know not just how big the
unnormalised density is around a point of interest but how this compares
to other parts of this space. Since we can only evaluate the density locally,
we have to somehow combine sparse local information to get a picture of the whole.
This becomes increasingly hard as the dimensionality of the problem grows.&lt;/p&gt;

&lt;h2 id=&quot;markov-chain-monte-carlo-mcmc-and-metropolis-hastings&quot;&gt;Markov Chain Monte Carlo (MCMC) and Metropolis-Hastings&lt;/h2&gt;
&lt;p&gt;Markov Chain Monte Carlo is a strategy for approximately solving the sampling problem.
MCMC works by simulating a particle roaming around the sampling space in such a way that,
in the limit of infinite exploration, the amount of time it spends in each part of the space
is proportional to the probability of that part of the space. In practice a good approximation
can be achieved after a finite amount of exploration.&lt;/p&gt;

&lt;h3 id=&quot;how-do-we-run-the-simulation&quot;&gt;How do we run the simulation?&lt;/h3&gt;
&lt;p&gt;We initialise our particle at some random point &lt;script type=&quot;math/tex&quot;&gt;X_0&lt;/script&gt; and then sample &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt; from
a carefully chosen proposal distribution &lt;script type=&quot;math/tex&quot;&gt;T(X_{t}|X_{t-1})&lt;/script&gt;. As long as we are
judicious in our choice of proposal we can prove that the particle will eventually
be a collection of samples from the target distribution &lt;script type=&quot;math/tex&quot;&gt;P_{target}(x)&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_0 \rightarrow X_1 \rightarrow X_2 \rightarrow ... \rightarrow X_t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_t) \rightarrow P_{target}&lt;/script&gt;

&lt;p&gt;In order for this to work we need our Markov transition distribution to have two
very important properties. First we need to know that our target distribution is a
fixed point of this transition operator. i.e. We need to know that if we sample a point
from &lt;script type=&quot;math/tex&quot;&gt;P(x)&lt;/script&gt; and then repeatedly sample from &lt;script type=&quot;math/tex&quot;&gt;T(X_{t}|X_{t-1})&lt;/script&gt; that the distribution
over our samples will marginally still be &lt;script type=&quot;math/tex&quot;&gt;P(x)&lt;/script&gt;. Formally we require:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int T(x'|x)P_{target}(x) dx = P_{target}(x')&lt;/script&gt;

&lt;p&gt;One sufficient (but not necessary) condition for this to be true is that &lt;script type=&quot;math/tex&quot;&gt;T(x'|x)&lt;/script&gt; satisfies
detailed balance:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{target}(x)T(x'|x) = P_{target}(x')T(x|x')&lt;/script&gt;

&lt;p&gt;If detailed balance holds than we are guaranteed that the target distribution is a fixed
point. You can see this by substituting the condition into the stationarity equation.
(The downside of detailed balance is that we’re always as likely to go forward in our simulation
as we are to go backwards and this slows us down)&lt;/p&gt;

&lt;p&gt;Once we’ve established that &lt;script type=&quot;math/tex&quot;&gt;P_{target}&lt;/script&gt; is a fixed point of our operator, we need to
know that no matter where we start we will end up at this fixed point. That is, we
need our chain of samples to be “ergodic”. We can guarantee ergodicity by making sure that
no point in the sample space is visited with a fixed period and no
parts of the sample space are inaccessible from each other.&lt;/p&gt;

&lt;h2 id=&quot;metropolis-hastings&quot;&gt;Metropolis-Hastings&lt;/h2&gt;
&lt;p&gt;The Metropolis-Hastings algorithm is a strategy for shoe-horning any Markov proposal
distribution into one that has the above desired properties. The way that this is
done is to introduce an accept-reject step at each stage of the simulation. Roughly
we sample from any Markovian transition operator &lt;script type=&quot;math/tex&quot;&gt;Q(x'|x)&lt;/script&gt; and then either accept
that point as our next sample or reject it, depending on how likely the proposed
point is under our target distribution. Concretely we sample &lt;script type=&quot;math/tex&quot;&gt;x' \sim Q(x'|x)&lt;/script&gt; and
then accept or reject with probability:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = min \{ \frac{P_{target}(x)}{P_{target}(x')} \frac{Q(x|x')}{Q(x'|x)}, 1 \}&lt;/script&gt;

&lt;p&gt;The transition from this combined process of sampling and rejecting is then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(x'|x) = Q(x'|x)\ min \{ \frac{P_{target}(x)}{P_{target}(x')} \frac{Q(x|x')}{Q(x'|x)}, 1 \}&lt;/script&gt;

&lt;p&gt;and this transition has the property, that NO MATTER what &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; is we have satisfied detailed balance!
This is because (assuming w.l.o.g the acceptance fraction is less than 1):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{target}(x)T(x'|x) = P_{target}(x) Q(x'|x) \frac{P_{target}(x')}{P_{target}(x)} \frac{Q(x|x')}{Q(x'|x)}
                        = Q(x|x')P_{target}(x')&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{target}(x')T(x|x') = P_{target}(x') Q(x|x') * 1&lt;/script&gt;

&lt;p&gt;so detailed balance holds.&lt;/p&gt;

&lt;p&gt;The power of Metropolis-Hastings is that it gives us enormous flexibility over our choice
of proposal distribution whilst still guaranteeing that we’ll eventually get accurate samples. This is
why it forms the back-bone of many popular sampling algorithms including the learning to sample methods
that I will talk about in my next post but also Gibbs sampling, Hamiltonain Monte-Carlo, NUTS and Slice
sampling.&lt;/p&gt;

&lt;p&gt;However, if we make a poor choice of &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; we will find that the probability of accepting a new point
is very low and it’ll take a very long time for our simulation to converge to the distribution of interest. Also,
we shoe-horn &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; into a valid transition operator by enforcing detailed balance and as was mentioned before,
detailed balance can seriously slow down the generation of samples.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;MCMC is one very powerful technique to solving the sampling problem. I personally find it pretty
amazing that a single point particle wandering around a high-dimensional sampling space can
gather enough information to give us a representative summary of the overall distribution.&lt;/p&gt;

&lt;p&gt;Whilst Metropolis-Hastings represents a good starting place for building sampling algorithms its performance
depends heavily on the choice of proposal distribution. MCMC only gives us exact samples in the limit of infinite
time, how good an approximation we get in finite time is very variable and actually quite hard to even assess.&lt;/p&gt;

&lt;p&gt;There are many variants of MCMC algorithms but a lot of them can be viewed as Metropolis-Hastings
with different carefully crafted choices for the proposal distribution &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;. One strategy that’s
recently been proposed has been to parameterise &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; and somehow learn the parameters of &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; to
provide efficient sampling for a given distribution.&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      </item>
    
      <item>
        <title>Autodiff Can Do Your Inference For You</title>
        <link>/autodiff0</link>
        <guid isPermaLink="true">/autodiff0</guid>
        <description>&lt;p&gt;Continuing with the theme of neat tricks that can be very useful in Machine Learning,
I wanted to share another insight that I was made aware of by &lt;a href=&quot;https://github.com/j-towns&quot;&gt;Jamie Townsend&lt;/a&gt;
who I think may have heard it from &lt;a href=&quot;http://people.csail.mit.edu/mattjj/&quot;&gt;Matthew Johnson&lt;/a&gt;. Jamie and Matthew are both contributors/authors of the Python &lt;a href=&quot;https://github.com/HIPS/autograd&quot;&gt;autograd&lt;/a&gt; package
which lets you differentiate native Python. Hugely useful for prototyping.&lt;/p&gt;

&lt;p&gt;The trick that Jamie pointed out to me is that for exponential family models, because the expected sufficient statistics
are related straightforwardly to the derivative of the log-normaliser, you can do a lot of your inference with no extra
effort by just leveraging automatic differentiation. I’ll very briefly recap the exponential family and the connection of derivatives to sufficient statistics before giving a quick example of just how easy this can be.&lt;/p&gt;

&lt;h3 id=&quot;exponential-family-distributions&quot;&gt;Exponential Family distributions&lt;/h3&gt;
&lt;p&gt;An exponential family distribution is just one that can be written in the following form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x|\theta) = g(\eta(\theta))f(x)e^{\eta(\theta)^T T(x)}&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; are the standard parameters of the distribution, &lt;script type=&quot;math/tex&quot;&gt;\eta(\theta)&lt;/script&gt; is a function of those parameters and is known as
the natural parameters and &lt;script type=&quot;math/tex&quot;&gt;T(x)&lt;/script&gt; are the sufficient statistics. &lt;script type=&quot;math/tex&quot;&gt;g(\theta)&lt;/script&gt; is simply the reciprocal of the normaliser.  Many if not most of the standard distributions we deal with can be written in this way eg. Normal, Poisson, Exponential, Laplacian, Bernoulli, Beta…&lt;/p&gt;

&lt;h3 id=&quot;the-derivatives-of-log-normaliser-are-the-expected-sufficient-stats&quot;&gt;The Derivatives of Log-normaliser are the Expected Sufficient Stats&lt;/h3&gt;

&lt;p&gt;There is a well known identity that says for the exponential family:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\nabla_{\eta} log(g(\eta)) = E_{p(x|\theta)}[T(x)]&lt;/script&gt;

&lt;p&gt;which is fairly straightforward to show as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\nabla_{\eta} log(g(\eta)) = g(\eta)\nabla_{\eta}\frac{1}{g(\eta)}&lt;/script&gt;

&lt;p&gt;and we know that &lt;script type=&quot;math/tex&quot;&gt;g(\eta)&lt;/script&gt; is the inverse of the normaliser:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{g(\eta)} = \int f(x)e^{\eta(\theta)^T T(x)} dx&lt;/script&gt;

&lt;p&gt;and so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\eta}\frac{1}{g(\eta)} = \int T(x) f(x)e^{\eta(\theta)^T T(x)} dx&lt;/script&gt;

&lt;p&gt;Thus:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\nabla_{\eta} log(g(\eta)) = g(\eta)\nabla_{\eta}\frac{1}{g(\eta)} =
\int T(x) g(\eta) f(x)e^{\eta(\theta)^T T(x)} dx = E_{p(x|\theta)}[T(x)]&lt;/script&gt;

&lt;h3 id=&quot;using-auto-diff-to-do-your-inference&quot;&gt;Using auto-diff to do your inference&lt;/h3&gt;

&lt;p&gt;Many times when doing inference of graphical models the expected sufficient statistics
are actually exactly what we want. For example when we do the EM algorithm over jointly exponential family
models, the E-step only requires us to find the expectations of the sufficient statistics under the posterior on the latents.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;https://github.com/HIPS/autograd/blob/master/examples/hmm_em.py&quot;&gt;autograd&lt;/a&gt; library, there is a nice example of taking advantage of this fact to do inference in an HMM.
Traditionally you would do your inference in an HMM using the forward-backward or Baum-Welch algorithm but the autograd library takes
advantage of the exponential family structure to do all the inference using differentiation.&lt;/p&gt;

&lt;p&gt;For an HMM with observed variables &lt;script type=&quot;math/tex&quot;&gt;y_{1:T}&lt;/script&gt;, discrete latents &lt;script type=&quot;math/tex&quot;&gt;x_{1:T}&lt;/script&gt; and
transition probabilities &lt;script type=&quot;math/tex&quot;&gt;p(y_t=j|x_t=i)=B_{ij}&lt;/script&gt;,
 &lt;script type=&quot;math/tex&quot;&gt;p(x_t|x_{t-1})=A_{ij}&lt;/script&gt;. The joint probability over all the &lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal{Y}&lt;/script&gt; is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;logP(\mathcal{X},\mathcal{Y}) = \sum_i x_0^i log \pi_i + \sum_t \sum_{i,j}
x_t^i x_{t+1}^j log A_{ij} + x_t^i y_t^j log B_{ij}&lt;/script&gt;

&lt;p&gt;where we have encoded the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; using one-hot vectors. This is clearly in the exponential family with sufficient statistics &lt;script type=&quot;math/tex&quot;&gt;\sum_t x_t^T x_{t+1}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\sum_t x_t^T y_{t}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To do inference what we need is the expectation of these statistics under
&lt;script type=&quot;math/tex&quot;&gt;P(\mathcal{X}|\mathcal{Y})&lt;/script&gt;. In fact all we really need is the log normaliser &lt;script type=&quot;math/tex&quot;&gt;Z = log \sum_x P(\mathcal{Y},\mathcal{X})&lt;/script&gt;.
Once we have the normaliser we can take its derivative and this will automatically take care of all our backwards message passing for us.
To calculate the log normaliser, we do the usual forwards pass and then sum all the forward messages.&lt;/p&gt;

&lt;p&gt;Thats exactly what the autograd library does in the following example. You can find the rest of the code &lt;a href=&quot;https://github.com/HIPS/autograd/blob/master/examples/hmm_em.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# First get the log partition function&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_partition_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;natural_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_partition_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;natural_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;log_pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;natural_params&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;log_alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_pi&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;log_alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logsumexp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logsumexp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;and then you can do all the inference simply using gradients:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;EM_update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;natural_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
       &lt;span class=&quot;c&quot;&gt;# E step&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;loglike&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;E_stats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_and_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_partition_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;natural_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loglike&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       &lt;span class=&quot;c&quot;&gt;# M step&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;E_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;why-does-this-work&quot;&gt;Why does this work?&lt;/h3&gt;

&lt;p&gt;At first it might seem a bit mysterious that we can replace a forwards and backwards
message passing algorithm by auto-diff alone but the reason this works is because
the sum product algorithm and reverse mode auto-diff are in fact very similar. Reverse mode
auto-diff first does a forward pass to calculate the value of the function and then retraces the computation graph
multiplying gradients along branches and summing at mergers in a backwards pass. This is highly analogous to the standard
forward backwards algorithm.&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Apr 2017 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>The Reparamaterisation Trick</title>
        <link>/reperam</link>
        <guid isPermaLink="true">/reperam</guid>
        <description>&lt;p&gt;Machine learning is full of lots of seemingly simple mathematical “tricks” that
have a disproportionate usefulness relative to their complexity. The Reparamaterisation
trick is one of these. In the next few posts I want to go into more detail about
variational auto-encoders and the trick will come up there again but for the time
being I just want to present a neat and useful “trick” that I think can be confusing
when you first come across it.&lt;/p&gt;

&lt;p&gt;Often times in machine learning we want to estimate the gradient of an expectation
using sampling. This happens in reinforcement learning when doing policy gradients and in stochastic
variational inference. The problem we are trying to solve is to find:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta E_{p(x)}[f(x)].&lt;/script&gt;

&lt;p&gt;As long as &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; is independent of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; there’s no problem here, we can
use Leibniz rule and simply bring the derivative into the expectation like follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{p(x)}[\nabla_\theta f_\theta(x)]&lt;/script&gt;

&lt;p&gt;and then construct a monte-carlo estimate for the gradient:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{p(x)}[\nabla_\theta f_\theta(x)] \approx \sum_i \nabla_\theta f_\theta(x_i)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is sampled from &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;. If however, as is often the case, the distribution
depends on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. i.e &lt;script type=&quot;math/tex&quot;&gt;P(x) = P(x|\theta)&lt;/script&gt;, then the above trick wont work because after
you bring the derivative inside the integral (or sum), the new integral is no longer an expectation
and we cant construct a straightforward monte-carlo estimate.&lt;/p&gt;

&lt;p&gt;There are two ways to get around this. The first is used in the Reinforce algorithm and is known
as the log derivative trick. It relies on using the following identity, which is easy to verify:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta g(\theta) \equiv g(\theta)\nabla_\theta log (g(\theta))&lt;/script&gt;

&lt;p&gt;If we substitute this identity for &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta p_\theta(x)&lt;/script&gt; in the following equation then we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta E_{p_\theta(x)}[f(x)] = \int f(x)\nabla_\theta p_\theta(x) dx = \int f(x)p_\theta(x)\nabla_\theta log p_\theta(x) dx = E_{p_{\theta}(x)}[f(x)\nabla_\theta log p_{\theta}(x)]&lt;/script&gt;

&lt;p&gt;and since this is still an expectation we can again construct a simple monte-carlo estimate of the gradient.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{p_{\theta}(x)}[f(x)\nabla_\theta log p_{\theta}(x)] \approx \sum_i f(x_i)\nabla_\theta log p_{\theta}(x_i)&lt;/script&gt;

&lt;p&gt;job done right? well sometimes yes but sometimes this naive substitution yields a gradient estimate that is too high
variance to be useful. In those cases we can turn to the reparamaterisation trick.&lt;/p&gt;

&lt;p&gt;The idea behind the reparamaterisation trick is write &lt;script type=&quot;math/tex&quot;&gt;x \sim p_\theta(x)&lt;/script&gt; as a function of a varible &lt;script type=&quot;math/tex&quot;&gt;z \sim q(z)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;q(z)&lt;/script&gt; does not depend
on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. If we can do this, then we are back in the first situation we considered where the gradient operator can simply be brought into
the expectation without any complexity. Put another way we want to find a differentiable function &lt;script type=&quot;math/tex&quot;&gt;\phi_\theta(z)&lt;/script&gt; such that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \phi_\theta(z)&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z \sim q(z)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is a distribution that has no &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; dependence. An example of such a function would
be the following reparamaterisation of a normal distribution &lt;script type=&quot;math/tex&quot;&gt;P_\theta(x) = N_x(\mu, \Sigma)&lt;/script&gt;, where what we were calling &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is
now &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;. In this case if we define:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi_\theta(z) = \mu + \Sigma^{\frac{1}{2}}z&lt;/script&gt;

&lt;p&gt;and then sample &lt;script type=&quot;math/tex&quot;&gt;z \sim q(z)= N_z(0,I)&lt;/script&gt;, we then have that &lt;script type=&quot;math/tex&quot;&gt;x = \phi_\theta(z)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Once we’ve made this reparamaterisation we can return to our original problem and see that the issues have disappeared.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_\theta E_{p_\theta(x)}[f(x)] = \nabla_\theta E_{q(z)}[f(\phi_\theta(z))] \approx \sum_i \nabla_\theta f(\phi_\theta(z))&lt;/script&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;z \sim q(z)&lt;/script&gt;. Or more concretely for our Gaussian case:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\mu,\Sigma} E_{N_x(\mu, \Sigma)}[f(x)] = \nabla_{\mu,\Sigma} E_{N_z(0,I)}[f(\mu + \Sigma^{\frac{1}{2}}z)] \approx \sum_i \nabla_{\mu,\Sigma} f(\mu + \Sigma^{\frac{1}{2}}z_i)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;z \sim N_z(0,I)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;It might at first seem that constructing these reparamaterisations might be very hard. In fact for discrete distributions, it is very
hard and in a later post we may discuss ways around this such as the Gumbel Softmax. However for continuos distributions when
you consider the fact that most random number generators first generate uniform distributed variables and
transform them, you realise that a lot of complex distributions can be built straightforwardly from simpler
distributions.&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Apr 2017 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>Intuition for Jensen's Inequality</title>
        <link>/jensen</link>
        <guid isPermaLink="true">/jensen</guid>
        <description>&lt;p&gt;I want to share some neat intuition that I came across in David Mackay’s text book
about Jensen’s inequality. Jensen’s inequality is a simple enough inequality that crops up frequently in
machine learning, especially in variational methods, when one wants to bound an
expectation.&lt;/p&gt;

&lt;p&gt;Jensen states that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[f(x)] \geq f(E[x])&lt;/script&gt;

&lt;p&gt;for any convex function f.&lt;/p&gt;

&lt;p&gt;Writing this out more verbosely it states (for discrete distributions):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum f(x_i) p_i  \geq f(\sum x_i p_i)&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;p_i = P(X = x_i)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now if we imagine that we place a mass of size &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; along the curve &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; at each of
the coordinates &lt;script type=&quot;math/tex&quot;&gt;(x_i, f(x_i))&lt;/script&gt; then &lt;script type=&quot;math/tex&quot;&gt;\sum f(x_i) p_i&lt;/script&gt; is simply the y-coordinate of the
centre of mass (remember &lt;script type=&quot;math/tex&quot;&gt;\sum p_i = 1&lt;/script&gt;). The &lt;script type=&quot;math/tex&quot;&gt;\sum x_i p_i&lt;/script&gt; is the x-coordinate.&lt;/p&gt;

&lt;p&gt;Viewed this way, all that Jensen’s inequality says is that for any convex curve, the centre of mass
of masses placed on the curve, must lie above the curve. That seems intuitively obvious to me in a way that
isn’t at first clear from staring at the inequality.&lt;/p&gt;

&lt;p&gt;The picture below shows some masses placed on a convex curve along with their centre of mass in 2-d.
It’s hopefully pretty obvious that the centre of mass of these masses has to lie above the curve (and its shown with a cross).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/jensenplot.jpg&quot; alt=&quot;my plot&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Apr 2017 00:00:00 +0100</pubDate>
      </item>
    
      <item>
        <title>Hello world!</title>
        <link>/hello-world</link>
        <guid isPermaLink="true">/hello-world</guid>
        <description>&lt;p&gt;Hello World!&lt;/p&gt;

&lt;p&gt;This is my first blog post. My name is Raza Habib and I’m a PhD student at UCL
studying Machine Learning under &lt;a href=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php&quot;&gt;David Barber&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’m interested in Artificial (General) Intelligence, Machine Learning, Physics and the Philosophy of
Science.&lt;/p&gt;

&lt;p&gt;This blog will be a mixture of shorter technical posts explaining important ideas
within machine learning interspersed with more speculative musings about the bigger picture
questions that arise when studying computational intelligence.&lt;/p&gt;

&lt;p&gt;The layout of this blog is heavily inspired by &lt;a href=&quot;http://dustintran.com/&quot;&gt;Dustin Tran’s&lt;/a&gt; blog.&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Apr 2017 00:00:00 +0100</pubDate>
      </item>
    
  </channel>
</rss>
